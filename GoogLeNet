#GoogLeNet

import torch
from torch import nn
from torch.nn import functional as F
from d2l import torch as d2l


#Inception块
class Inception(nn.Module):
    #通道定义
    def __init__(self,in_channels,c1,c2,c3,c4,**kwargs):
        super(Inception,self).__init__(**kwargs)
        self.p1_1 = nn.Conv2d(in_channels,c1,kernel_size=1) #线路1：单1*1卷积层
        self.p2_1 = nn.Conv2d(in_channels,c2[0],kernel_size=1)
        self.p2_2 = nn.Conv2d(c2[0],c2[1],kernel_size=3,padding=1)#线路2：1*1后3*3卷积层,填充1
        self.p3_1 = nn.Conv2d(in_channels,c3[0],kernel_size=1)
        self.p3_2 = nn.Conv2d(c3[0],c3[1],kernel_size=5,padding=2)#线路3:1*1后5*5卷积层，填充2
        self.p4_1 = nn.MaxPool2d(kernel_size=3,stride=1,padding=1)
        self.p4_2 = nn.Conv2d(in_channels,c4,kernel_size=1)#线路4:3*3最大汇聚层后1*1卷积层

    #通道合并
    def forward(self,x):
        p1 = F.relu(self.p1_1(x))
        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))
        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))
        p4 = F.relu(self.p4_2(self.p4_1(x)))
        #在通道维度上连结输出
        return torch.cat((p1,p2,p3,p4),dim=1)


#module
#第一个模块使用64个通道，7*7卷积层
b1 = nn.Sequential(nn.Conv2d(1,64,kernel_size=7,stride=2,padding=3),
                   nn.ReLU(),
                   nn.MaxPool2d(kernel_size=3,stride=2,padding=1))
#第二个模块使用两个卷积层：1.64通道，1*1卷积层，2.通道数增加三倍的3*3卷积层
b2 = nn.Sequential(nn.Conv2d(64,64,kernel_size=1),
                   nn.ReLU(),
                   nn.Conv2d(64,192,kernel_siz=3,padding=1),
                   nn.ReLU(),
                   nn.MaxPool2d(kernel_siz3=3,stride=2,padding=1))
#第三个模块串联2个Inception块
b3 = nn.Sequential(Inception(192,64,(96,128),(16,32),32),#64+128+32+32=256通道数
                   Inception(256,128,(128,192),(32,96),64),#128+192+96+64=480输出通道数
                   nn.MaxPool2d(kernel_siz=3,stride=2,padding=1))
#第四个模块串联5个Inception块
b4 = nn.Sequential(Inception(480,192,(96,208),(16,48),64),
                   Inception(512, 160, (112, 224), (24, 64), 64),
                   Inception(512, 128, (128, 256), (24, 64), 64),
                   Inception(512, 112, (144, 288), (32, 64), 64),
                   Inception(528, 256, (160, 320), (32, 128), 128),
                   nn.MaxPool2d(kernel_size=3, stride=2, padding=1))
#第五个模块包含两个Inception块，后面紧跟输出层，该模块同NiN一样使用全局平均汇聚层，将每个通道的高和宽变成1。    
b5 = nn.Sequential(Inception(832,256,(160,320),(32,128),128),
                   Inception(832,384,(192,384),(48,128),128),
                   nn.AdaptiveAvgPool2d((1,1)),
                   nn.Flatten())
#最后我们将输出变成二维数组，再接上一个输出个数为标签类别数的全连接层。
net = nn.Sequential((b1,b2,b3,b4,b5,nn.Linear(1024,10)))               

#演示各个模块输出的形状变化
X = torch.rand(size=(1,1,96,96))
for layer in net:
    X = layer(X)
    print(layer.__class__.__name__,'output shape:\t',X.shape)

#训练模型

#转换为96*96分辨率
lr,num_epochs,batch_size =0.1,10,128
train_iter,tesr_iter = d2l.load_data_fashion_mnist(batch_size,resize=96)
d2l.train_ch6(net,train_iter,test_iter,num_epochs,lr,d2l.try_gpu())
